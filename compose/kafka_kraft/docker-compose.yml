version: "3.8" 

# docker compose 规定配置需要以x-开头
x-kafka: &kfk-server
  image: docker.io/bitnami/kafka:3.5
  # 镜像指定了用1001:root用户，需要更改，否则挂载的目录是属于root用户的，会导致没权限无法启动
  user: "root"
  # &kfk-server锚点引用覆盖会将整个environment元素删除替换新的元素，而不是合并
  environment: &kfk-env
    ALLOW_PLAINTEXT_LISTENER: yes
    # KRaft settings
    KAFKA_CFG_PROCESS_ROLES: controller,broker
    # node id + hostname + port
    KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka0:9093,1@kafka1:9093,2@kafka2:9093
    # Listeners
    KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
    KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
    KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
    KAFKA_CFG_INTER_BROKER_LISTENER_NAME: PLAINTEXT
    KAFKA_KRAFT_CLUSTER_ID: abcdefghijklmnopqrstuv
    KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: true
  networks: 
    - kafka-cluster
  healthcheck:
    # test: nc -z localhost 2181 || exit 1
    test: ["CMD", "/opt/bitnami/kafka/bin/kafka-topics.sh", "--bootstrap-server", "localhost:9092", "--list"]
    interval: 10s
    timeout: 5s
    retries: 30
services:
  # 使用yaml的锚点语法建立模板
  kafka0: 
    << : *kfk-server
    hostname: kafka0
    container_name: kafka0
    # ports:
    #   - "19092:19092"
    # 外挂磁盘，测试删除数据恢复
    volumes: 
      - ./data/kafka0:/bitnami/kafka
    environment: 
      << : *kfk-env
      KAFKA_CFG_NODE_ID: 0
      # 启动主机外部访问，主机外部访问需要使用不同端口，并且需要配置host文件 127.0.0.1 kafka0
      # KAFKA_CFG_LISTENERS: PLAINTEXT://:19092,CONTROLLER://:9093
      # KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://:19092
  kafka1:
    # 使用yaml的锚点语法引用锚点
    << : *kfk-server
    hostname: kafka1
    container_name: kafka1
    # ports:
    #   - "19093:19093"
    volumes: 
      - ./data/kafka1:/bitnami/kafka
    environment: 
      << : *kfk-env
      KAFKA_CFG_NODE_ID: 1
      # 启动主机外部访问，主机外部访问需要使用不同端口，并且需要配置host文件 127.0.0.1 kafka1
      # KAFKA_CFG_LISTENERS: PLAINTEXT://:19093,CONTROLLER://:9093
      # KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://:19093
  kafka2:
    << : *kfk-server
    hostname: kafka2
    container_name: kafka2
    # ports:
    #   - "19094:19094"
    volumes: 
      - ./data/kafka2:/bitnami/kafka
    environment: 
      << : *kfk-env
      KAFKA_CFG_NODE_ID: 2
      # 启动主机外部访问，主机外部访问需要使用不同端口，并且需要配置host文件 127.0.0.1 kafka2
      # KAFKA_CFG_LISTENERS: PLAINTEXT://:19094,CONTROLLER://:9093
      # KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://:19094

  kafka-exporter:
    image: docker.io/bitnami/kafka-exporter:1.7.0
    user: "root"
    command: ["--kafka.server=kafka0:9092", "--kafka.server=kafka1:9092", "--kafka.server=kafka2:9092"]
    hostname: kafka-exporter
    container_name: kafka-exporter
    ports:
      - 9308:9308
    networks: 
      - kafka-cluster
    depends_on: 
      kafka0: 
        condition: service_healthy
      kafka1: 
        condition: service_healthy
      kafka2: 
        condition: service_healthy

  prometheus:
    image: bitnami/prometheus:2.48.1
    # user: "root"
    hostname: prometheus
    container_name: prometheus
    volumes: 
      - prometheus_v:/opt/bitnami/prometheus/data
      # 配置kafka exporter地址
      - ./config/prometheus.yml:/opt/bitnami/prometheus/conf/prometheus.yml
    ports:
      - 9090:9090
    depends_on: 
      - kafka-exporter
    networks: 
      - kafka-cluster

  grafana:
    # image: docker.io/bitnami/grafana:10.2.3
    image: grafana/grafana:10.0.10
    # user: "root"
    hostname: grafana
    container_name: grafana
    ports:
      - '3000:3000'
    volumes:
      # - ./data/grafana:/var/lib/grafana
      - grafana_v:/var/lib/grafana
    networks: 
      - kafka-cluster

  # grafana:
  #   image: docker.io/bitnami/grafana:10.2.3
  #   # user: "root"
  #   hostname: grafana
  #   container_name: grafana
  #   ports:
  #     - '3000:3000'
  #   environment:
  #     # 用户名 admin
  #     - 'GF_SECURITY_ADMIN_PASSWORD=bitnami'
  #   # volumes:
  #     # - ./data/grafana:/opt/bitnami/grafana/data
  #     # - grafana_v:/opt/bitnami/grafana1/data1
  #   networks: 
  #     - kafka-cluster

  # ksqldb-server:
  #   image: confluentinc/ksqldb-server:0.29.0
  #   hostname: ksqldb-server
  #   container_name: ksqldb-server
  #   depends_on:
  #     - kafka0
  #     - kafka1
  #     - kafka2
  #   ports:
  #     - "8088:8088"
  #   environment:
  #     KSQL_LISTENERS: http://0.0.0.0:8088
  #     KSQL_BOOTSTRAP_SERVERS: kafka0:9092,kafka1:9092,kafka2:9092
  #     KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
  #     KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"

  # ksqldb-cli:
  #   image: confluentinc/ksqldb-cli:0.29.0
  #   container_name: ksqldb-cli
  #   depends_on:
  #     - kafka0
  #     - kafka1
  #     - kafka2
  #     - ksqldb-server
  #   entrypoint: /bin/sh
  #   tty: true
      
  # ksql:
  #   hostname: kafka2
  #   image: bitnami/ksql:7.1.11
  #   environment: 
  #     KAFKA_CFG_NODE_ID: 2

  # magic:
  #   image: "digitsy/kafka-magic:4.0.1.138"
  #   ports:
  #     - "8081:80"
  #   volumes:
  #     - ./data/magic:/config
  #   environment:
  #     KMAGIC_ALLOW_TOPIC_DELETE: "true"
  #     KMAGIC_ALLOW_SCHEMA_DELETE: "true"
  #     KMAGIC_CONFIG_STORE_TYPE: "file"
  #     KMAGIC_CONFIG_STORE_CONNECTION: "Data Source=/config/KafkaMagicConfig.db;"
  #     KMAGIC_CONFIG_ENCRYPTION_KEY: "ENTER_YOUR_KEY_HERE"
  #   networks: 
  #     - kafka-cluster
  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
      KAFKA_CLUSTERS_0_NAME: kafka_test
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka0:9092,kafka1:9092,kafka2:9092
      # KAFKA_CLUSTERS_0_READONLY: true
    # volumes:
    #   - ~/kui/config.yml:/etc/kafkaui/dynamic_config.yaml
    networks: 
      - kafka-cluster

networks:
  kafka-cluster: 
    name: kafka-cluster
    ipam:
      driver: default
      config: 
        - subnet: 192.168.164.0/20
volumes:
#   kafka_0_data:
#     driver: local
#   kafka_1_data:
#     driver: local
#   kafka_2_data:
#     driver: local
  prometheus_v:
    driver: local
  grafana_v:
    driver: local