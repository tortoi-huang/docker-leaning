# 启动一个本地kafka集群，本地的host文件要配置kafka主机名指向暴露端口，否则无法连接，因为连接到服务器后返回的各个节点的地址是主机名称如：
# 127.0.0.1 kafka0
# 127.0.0.1 kafka2
# 127.0.0.1 kafka1
version: "3.8" 

# docker compose 规定配置需要以x-开头
x-kafka: &kfk-server
  image: docker.io/bitnami/kafka:3.5
  # &kfk-server锚点引用覆盖会将整个environment元素删除替换新的元素，而不是合并
  environment: &kfk-env
    ALLOW_PLAINTEXT_LISTENER: yes
    # KRaft settings
    KAFKA_CFG_PROCESS_ROLES: controller,broker
    # node id + hostname + port
    KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka0:9093,1@kafka1:9093,2@kafka2:9093
    # Listeners
    KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
    KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
    KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
    KAFKA_CFG_INTER_BROKER_LISTENER_NAME: PLAINTEXT
    KAFKA_KRAFT_CLUSTER_ID: abcdefghijklmnopqrstuv
    KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: true
  networks: 
    - kafka-cluster
services:
  # 使用yaml的锚点语法建立模板
  kafka0: 
    << : *kfk-server
    hostname: kafka0
    # ports:
    #   - "19092:19092"
    # 外挂磁盘，测试删除数据恢复
    volumes: 
      - ./data/kafka0:/bitnami/kafka
    environment: 
      << : *kfk-env
      KAFKA_CFG_NODE_ID: 0
      # KAFKA_CFG_LISTENERS: PLAINTEXT://:19092,CONTROLLER://:9093
      # KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://:19092
  kafka1:
    # 使用yaml的锚点语法引用锚点
    << : *kfk-server
    hostname: kafka1
    # ports:
    #   - "19093:19093"
    volumes: 
      - ./data/kafka1:/bitnami/kafka
    environment: 
      << : *kfk-env
      KAFKA_CFG_NODE_ID: 1
      # KAFKA_CFG_LISTENERS: PLAINTEXT://:19093,CONTROLLER://:9093
      # KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://:19093
  kafka2:
    << : *kfk-server
    hostname: kafka2
    # ports:
    #   - "19094:19094"
    volumes: 
      - ./data/kafka2:/bitnami/kafka
    environment: 
      << : *kfk-env
      KAFKA_CFG_NODE_ID: 2
      # KAFKA_CFG_LISTENERS: PLAINTEXT://:19094,CONTROLLER://:9093
      # KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://:19094

  # ksqldb-server:
  #   image: confluentinc/ksqldb-server:0.29.0
  #   hostname: ksqldb-server
  #   container_name: ksqldb-server
  #   depends_on:
  #     - kafka0
  #     - kafka1
  #     - kafka2
  #   ports:
  #     - "8088:8088"
  #   environment:
  #     KSQL_LISTENERS: http://0.0.0.0:8088
  #     KSQL_BOOTSTRAP_SERVERS: kafka0:9092,kafka1:9092,kafka2:9092
  #     KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
  #     KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"

  # ksqldb-cli:
  #   image: confluentinc/ksqldb-cli:0.29.0
  #   container_name: ksqldb-cli
  #   depends_on:
  #     - kafka0
  #     - kafka1
  #     - kafka2
  #     - ksqldb-server
  #   entrypoint: /bin/sh
  #   tty: true
      
  # ksql:
  #   hostname: kafka2
  #   image: bitnami/ksql:7.1.11
  #   environment: 
  #     KAFKA_CFG_NODE_ID: 2

  # magic:
  #   image: "digitsy/kafka-magic:4.0.1.138"
  #   ports:
  #     - "8081:80"
  #   volumes:
  #     - ./data/magic:/config
  #   environment:
  #     KMAGIC_ALLOW_TOPIC_DELETE: "true"
  #     KMAGIC_ALLOW_SCHEMA_DELETE: "true"
  #     KMAGIC_CONFIG_STORE_TYPE: "file"
  #     KMAGIC_CONFIG_STORE_CONNECTION: "Data Source=/config/KafkaMagicConfig.db;"
  #     KMAGIC_CONFIG_ENCRYPTION_KEY: "ENTER_YOUR_KEY_HERE"
  #   networks: 
  #     - kafka-cluster
  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
      KAFKA_CLUSTERS_0_NAME: kafka_test
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka0:9092,kafka1:9092,kafka2:9092
      KAFKA_CLUSTERS_0_READONLY: true
    # volumes:
    #   - ~/kui/config.yml:/etc/kafkaui/dynamic_config.yaml
    networks: 
      - kafka-cluster

networks:
  kafka-cluster: 
    name: kafka-cluster
    ipam:
      driver: default
      config: 
        - subnet: 192.168.164.0/20
# volumes:
#   kafka_0_data:
#     driver: local
#   kafka_1_data:
#     driver: local
#   kafka_2_data:
#     driver: local